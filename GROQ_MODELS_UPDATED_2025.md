# Groq Models - Updated January 2025

## ğŸš¨ Important: Model Deprecation

**mixtral-8x7b-32768** has been **DECOMMISSIONED** by Groq!

## âœ… Currently Available Groq Models (Free Tier)

### 1. llama-3.3-70b-versatile â­ RECOMMENDED
**Best for: Quality responses**
- **Quality:** â­â­â­â­â­ (Highest quality)
- **Speed:** âš¡âš¡âš¡ (Moderate)
- **Limits:**
  - 6,000 tokens per minute (TPM)
  - 30 requests per minute (RPM)
  - **100,000 tokens per day (TPD)** âš ï¸
- **Context:** 8,192 tokens
- **Use case:** When you need the best answers (but watch daily limit!)

### 2. llama-3.1-8b-instant
**Best for: Fast responses**
- **Quality:** â­â­â­ (Good)
- **Speed:** âš¡âš¡âš¡âš¡âš¡ (Fastest!)
- **Limits:**
  - 6,000 tokens per minute (TPM)
  - 30 requests per minute (RPM)
  - 14,400 requests per day
- **Context:** 8,192 tokens
- **Use case:** Quick conversations, faster responses

### 3. llama-3.1-70b-versatile
**Best for: Balanced performance**
- **Quality:** â­â­â­â­ (Very Good)
- **Speed:** âš¡âš¡âš¡âš¡ (Fast)
- **Limits:**
  - 6,000 tokens per minute (TPM)
  - 30 requests per minute (RPM)
  - 14,400 requests per day
- **Context:** 8,192 tokens
- **Use case:** Good middle ground

### 4. gemma2-9b-it
**Best for: Higher rate limits**
- **Quality:** â­â­â­ (Good)
- **Speed:** âš¡âš¡âš¡ (Moderate)
- **Limits:**
  - **15,000 tokens per minute (TPM)** âœ¨ (Best!)
  - 30 requests per minute (RPM)
  - No daily token limit
- **Context:** 8,192 tokens
- **Use case:** When you hit rate limits with other models

---

## ğŸ¯ My New Recommendations

### Option 1: Best Quality â†’ llama-3.3-70b-versatile
**Use if:** You want the smartest responses and don't chat too much

**Pros:**
- âœ… Highest quality responses
- âœ… Best reasoning

**Cons:**
- âŒ Daily limit of 100k tokens (runs out if you chat a lot)
- âŒ Slower than instant

**How to set:**
```
Settings â†’ Model: llama-3.3-70b-versatile
```

### Option 2: Best for Heavy Use â†’ gemma2-9b-it
**Use if:** You chat a LOT and hit rate limits

**Pros:**
- âœ… **15,000 TPM** (2.5x higher than others!)
- âœ… No daily token limit
- âœ… Good enough quality

**Cons:**
- âŒ Not as smart as Llama 3.3
- âŒ Moderate speed

**How to set:**
```
Settings â†’ Model: gemma2-9b-it
```

### Option 3: Fastest Responses â†’ llama-3.1-8b-instant
**Use if:** You want super fast responses

**Pros:**
- âœ… Fastest model available
- âœ… Good for quick questions

**Cons:**
- âŒ Lower quality than 70b models
- âŒ 6,000 TPM limit
- âŒ 14,400 request/day limit

**How to set:**
```
Settings â†’ Model: llama-3.1-8b-instant
```

---

## ğŸ“Š Comparison Table

| Model | Quality | Speed | TPM | Daily Limit | Best For |
|-------|---------|-------|-----|-------------|----------|
| **llama-3.3-70b-versatile** | â­â­â­â­â­ | âš¡âš¡âš¡ | 6,000 | 100k tokens | Quality |
| **llama-3.1-70b-versatile** | â­â­â­â­ | âš¡âš¡âš¡âš¡ | 6,000 | 14.4k req | Balanced |
| **llama-3.1-8b-instant** | â­â­â­ | âš¡âš¡âš¡âš¡âš¡ | 6,000 | 14.4k req | Speed |
| **gemma2-9b-it** | â­â­â­ | âš¡âš¡âš¡ | **15,000** | None | Heavy use |

---

## ğŸ”§ Rate Limiting Protection

I've already added automatic rate limiting to your app:
- **Waits 4 seconds between Groq requests**
- Prevents 429 rate limit errors
- You'll see: `â³ Rate limit protection: Waiting 3s...`

This helps stay within the per-minute limits!

---

## ğŸ’¡ Still Getting Rate Limits?

### Solution 1: Use Ollama (Unlimited!)
```bash
# Install Ollama
# Download from: https://ollama.com/download

# Pull a model
ollama pull llama3.2

# In Seven AI:
Settings â†’ Provider: Ollama
Settings â†’ Model: llama3.2
```

**Benefits:**
- âœ… Truly unlimited (runs on your computer)
- âœ… Works offline
- âœ… No rate limits
- âœ… Free forever

### Solution 2: Upgrade Groq to Paid
- **$0.50 per 1M tokens** for Llama models
- Much higher rate limits
- https://console.groq.com/settings/billing

### Solution 3: Use Multiple Providers
Switch between providers when you hit limits:
- **Groq** â†’ Use until rate limit
- **Ollama** â†’ Switch when offline or hit limits
- **OpenAI** â†’ Premium option (paid)

---

## ğŸ¯ What to Do Right Now

### Step 1: Pick a Model

**If you want quality:**
```
llama-3.3-70b-versatile
```

**If you chat a lot:**
```
gemma2-9b-it
```

**If you want speed:**
```
llama-3.1-8b-instant
```

### Step 2: Update Settings
1. Open Seven AI
2. Click Settings
3. Find "Model" field
4. Enter one of the models above
5. Click **ğŸ’¾ Save Model Settings**

### Step 3: Test It
Ask Seven a question and verify it works!

---

## ğŸ“š Official Groq Documentation

For the most up-to-date information:
- **Models:** https://console.groq.com/docs/models
- **Rate Limits:** https://console.groq.com/docs/rate-limits
- **Deprecations:** https://console.groq.com/docs/deprecations

---

## âŒ Deprecated/Removed Models

These models NO LONGER WORK:
- âŒ mixtral-8x7b-32768 (decommissioned)
- âŒ llama-3.3-70b-versatile might have older versions

Always check the official docs for the latest!

---

## ğŸ¯ My Final Recommendation

**For most users:**
Use **`llama-3.3-70b-versatile`** for best quality, and if you hit the daily limit, switch to **`gemma2-9b-it`** for the rest of the day.

**Or just use Ollama** for unlimited, free, local AI! ğŸš€

---

## âœ… Files Updated

- âœ… `src/ui/components/Settings.tsx` - Updated model recommendations
- âœ… `GROQ_MODELS_UPDATED_2025.md` - This guide

---

Sorry for recommending a dead model! These should all work now. ğŸ™







